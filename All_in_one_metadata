
                    
//So lets get started by installing the required dependencies.Make sure you have them on your system.

                                ***** Software Installation *****

//Install Python, TensorFlow and some IDE (Jupyter, TensorFlow, etc.)

//Installing Python and TensorFlow
--> It is possible to install and run Python/TensorFlow entirely from your own computer. Google provides TensorFlow for Windows, Mac and Linux. Previously, TensorFlow did not support Windows. However, as of December 2016, TensorFlow supports Windows for both CPU and GPU operation.

//The first step is to install Python 3.6.(rather try installing the latest version)
-->it is easy to install Jupyter notebooks with the following command:
     conda install jupyter

//Once Jupyter is installed, it is started with the following command:
  --> jupyter notebook

//The following packages are needed for this Project typo...

conda install scipy
pip install sklearn
pip install pandas
pip install pandas-datareader
pip install matplotlib
pip install pillow
pip install requests
pip install h5py
pip install tensorflow==1.8.0
pip install keras==2.2.0

**Imp: I've insatlled a specific version of tensorflow.One can install the latest version as well,just make sure that the dependencies work properly with the latest version.
// Also installing the dependencies might take time so be patient.

// Open the jupyter notebook by typing " jupyter notebook" in the terminal.
// A new local web window will open in your default browser.
//select the folder which contains the dataset "creditcard.csv".
//Go to the top right corner and click on new >> python3.(This will create a new python3 blank notebook)  
//Rename the file and you're all set to go....!
                    HAPPY MACHINE LEARNING :)    --> And yeah Android still sucks #SwitchToiPhone

STEP-1

import sys
import numpy
import pandas
import matplotlib 
import seaborn
import sklearn
import scipy

print('Python :{}'.format(sys.version))
print('Numpy :{}'.format(numpy.__version__))
print('Pandas :{}'.format(pandas.__version__))
print('Matplotlib :{}'.format(matplotlib._version_))
print('Seaborn :{}'.format(seaborn.__version__))
print('Scipy :{}'.format(scipy.__version__))
print('Sklearn :{}'.format(sklearn.__version__))

//press shift + enter to see this code in action.( this prints the versions of the imported dependencies i.e packages.)

STEP-2

#setting up the dependencies.....(lets kill it fella! ;)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 

// PRESS shift+enter after typing this code.Also make sure that the section returns a number once the code is executed.(left hand side of the section returns a number)

STEP-3
# lets import the fu*kin database...
data = pd.read_csv('creditcard.csv')

**THIS WILL IMPORT THE DATASET THAT WE DOWNLOADED FROM Kaggle.com. (Dont forget to press shift+ enter after each and every step)


Step-4

print(data.columns)     // prints all the column names in the dataset
print(data.shape)       // prints the structure of the dataset i.e columns X rows
print(data.describe())  // prints the schema of the dataset similar to what a "describe" command in sql does) 
 
 * Shift+enter is mandatory
 
Step-5

data = data.sample(frac = 0.5, random_state =1)
print(data.shape)
                
                    Continue the shift + enter ritual :)
                    
** here we will be working with the data in samples " frac = 0.5 means using onlt the 50% of the data from the entire set.One s free to experiment with the size of the sample.
Also random_state =1 is just a jargon as in a stste specifier ( dont pay much attention to it at the moment)

STEP-5

data.hist(figsize = (20,20))
plt.show()

            Goes without saying press " Shift + enter "
            
** jupyter notebook and the matplotlib package that we imported forms a deadly combination for ploting graphs.
here (figsize = (20,20)) are the dimensions of the histogram that we want to plot. one is free to experiment with the dimensions,
just remember the higher the dimensions the more time it will take to plot the histogram.


Step-6

Fraud = data[data['Class']== 1]
Valid = data[data['Class']== 0]

outlier_fraction = len(Fraud) / float(len(Valid))
print(outlier_fraction)

print('Fraud cases :{}'.format(len(Fraud)))
print('Valid cases :{}'.format(len(Valid)))
  
           press: Shift + enter
** this part of code deals with counting the fraud cases and the fair cases and the outlier fraction from the dataset sample that we're using.


Step-7

#Co-fu*kin-relation matrix.... is there something cooking between them? Lets find out

cor_mat = data.corr()
fig = plt.figure(figsize = (12,9))
sns.heatmap(cor_mat, vmax = .8, square = True)

** this part of the code gives us the co-relation matrix between the dataset.
Also the "seaborn" package that we imported earlier will plot a  " heatmap ". This technique of data visualisation helps one understand the behaviour of data
with several other factors. (understanding the heatmap isn't that easy so try and spend some time reading about it)


Step-8

#get all the columns from the dataframe.....
columns = data.columns.tolist()


#lets filter the columns to remove data we don't want .. in short lets clean the shit up #Swatch_Bharat_Abhiyan
columns = [ c for c in columns if c  not in ["Class"] ]

# the predection varibale 

Steve = "Class"

A = data[columns]
B = data[Steve]

print(A.shape)
print(B.shape)
                Shift + enter is mandatory...
** Here the only thing to pay attention on is the for loop. I've defined a class named steve which will be our prediction variable
rest everything is basic python syntax.This wil return our dataset structure but without the top row b'cuz we dont need it anymore.

from sklearn.matrics import classification_report,accuracy_score
from sklern.ensemble import IsolationForest
from sklearn.neighbours import LocalOutlierFactor
9

Step-8
#lets define a random state...
state = 1

# finally the main part:- outlier detection methods....
classifiers = {
    "Isolation Forest": IsolationForest(max_samples=len(A),
                                      contamination = outlier_fraction,
                                       random_state= state)
     "Local Outlier Factor": LocalOutlierFactor(
     n_neighbours= 30,
     contamination = outlier_fraction)                                 
}

** here we're using a Machine learning algorithm " isolation forest", one can use other algorithm as well if that works well for larger data sets.
Machine Learning is all about selection of the right algorithm to produce results in the fastest time.
Also contamination here is for the fraud cases that will be encountered in the processing of the dataset by isolation forest.(works similar to KNN,
BUT I ON A PERSONAL NOTE LIKE ISOLATION FOREST cuz its not mainstream :-) )


Step-10

#final task(maybe)..........
n_outliers = len(Fraud)

for i, (clf_name, clf) in enumerate(classifiers.items()):
    
#set and fit the data & mark outliers...
    
        if clf_name == "Local Outlier Factor":
            a_pred= clf.fit_predict(A)
            scores_pred= clf.negative_outlier_factor_
            
        else:
            clf.fit = (A)
            scores_pred= clf.decision_function(A)
            a_pred= clf.predict(A)
        
        
#reshaping the prediction values , 0 for valid and 1 for Fraud.....

        a_pred[a_pred == 1]== 0
        a_pred[a_pred == -1]== 1
        
        n_errors = (a_pred != A).sum()
        
        #Run the classifier matrices
        
        print('{}: {}'.format(clf_name,n_errors))
        print(accuracy_score(A,a_pred))
        print(classification_report(A,a_pred))
        
 **Lets not describe this code segment at the moment hit shift+ enter and explore the outcome.
#AvanIsHere 




** Learning Credits : https://github.com/jeffheaton 
I would like to express my Sincere gratitude for Mr. jeffheaton for making such great learning material availabe. I do owe you big time sir :)

All Rights Reserved
copyright : Avan Avi
2018 standard MIT license
