# -*- coding: utf-8 -*-
"""Credit_Card_Fraud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bVzhBGeIiUG2TOMe5EbFMRAQL5bnaiOI
"""

import sys
import numpy
import pandas
import matplotlib
import seaborn
import sklearn
import scipy

print('Python :{}'.format(sys.version))
print('Numpy :{}'.format(numpy.__version__))
print('Pandas :{}'.format(pandas.__version__))
print('Matplotlib :{}'.format(matplotlib.__version__))
print('Seaborn :{}'.format(seaborn.__version__))
print('Scipy :{}'.format(scipy.__version__))
print('Sklearn :{}'.format(sklearn.__version__))

# Setting up the dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd

data = pd.read_csv('/content/creditcard.csv')

print(data.columns)     // prints all the column names in the dataset
print(data.shape)       // prints the structure of the dataset i.e columns X rows
print(data.describe())  // prints the schema of the dataset

# prints all the column names in the dataset
print(data.columns)

print(data.shape)

print(data.describe())

data = data.sample(frac = 0.5, random_state =1)
print(data.shape)

data.hist(figsize = (20,20))
plt.show()

Fraud = data[data['Class']== 1]
Valid = data[data['Class']== 0]

outlier_fraction = len(Fraud) / float(len(Valid))
print(outlier_fraction)

print('Fraud cases :{}'.format(len(Fraud)))
print('Valid cases :{}'.format(len(Valid)))

# Correlation matrix
cor_mat = data.corr()
fig = plt.figure(figsize = (12,9))
sns.heatmap(cor_mat, vmax = .8, square = True)

#get all the columns from the dataframe
columns = data.columns.tolist()

# Filter the columns to remove data we don't want
columns = [c for c in columns if c not in ["Class"]]

# The prediction variable
target = "Class"

X = data[columns]
Y = data[target]

print(X.shape)
print(Y.shape)

from sklearn.metrics import classification_report,accuracy_score
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor

# Define a random state
state = 1

# Outlier detection methods
classifiers = {
    "Isolation Forest": IsolationForest(max_samples=len(X),
                                      contamination = outlier_fraction,
                                       random_state= state),
     "Local Outlier Factor": LocalOutlierFactor(
     n_neighbors= 20,
     contamination = outlier_fraction)
}

from sklearn.impute import SimpleImputer

# use mean imputation
imputer = SimpleImputer(strategy='mean')

# fit on the dataset
imputer = imputer.fit(X)

# transform the dataset
X = imputer.transform(X)

X_df = pd.DataFrame(X)
X_df = X_df.dropna()
X = X_df.to_numpy()  # If you need to convert it back to a numpy array

# Drop any row in your dataframe that contains at least one NaN
data = data.dropna()

# Then create your X and Y arrays
X = data.drop('Class', axis=1)
Y = data['Class']

X = X.values
Y = Y.values

# Number of outliers
n_outliers = len(Fraud)

for i, (clf_name, clf) in enumerate(classifiers.items()):

    # Fit the data and tag outliers
    if clf_name == "Local Outlier Factor":
        y_pred = clf.fit_predict(X)
        scores_pred = clf.negative_outlier_factor_
    else:
        clf.fit(X)
        scores_pred = clf.decision_function(X)
        y_pred = clf.predict(X)

    # Reshape the prediction values to 0 for valid and 1 for fraud
    y_pred[y_pred == 1] = 0
    y_pred[y_pred == -1] = 1

    n_errors = (y_pred != Y).sum()

    # Run classification metrics
    print('{}: {}'.format(clf_name,n_errors))
    print(accuracy_score(Y,y_pred))
    print(classification_report(Y,y_pred))

